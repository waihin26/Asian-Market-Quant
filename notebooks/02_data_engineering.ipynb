{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b14f7b3a",
   "metadata": {},
   "source": [
    "# Asian Market Quant Project\n",
    "\n",
    "## 2. Data Engineering Pipeline\n",
    "\n",
    "**Author:** Wong Wai Hin  \n",
    "**Date:** July 20, 2025\n",
    "\n",
    "This notebook demonstrates the data engineering pipeline for the Asian Market Quant project, focusing on:\n",
    "\n",
    "1. Data ingestion and cleaning\n",
    "2. Currency normalization to USD\n",
    "3. Corporate actions and futures roll handling\n",
    "4. Calculation of returns\n",
    "5. Data dictionary generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acab60cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries and modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add project root to path for imports\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import our data engineering module\n",
    "from src.data_engineering import (\n",
    "    load_raw_data,\n",
    "    clean_and_standardize,\n",
    "    normalize_to_usd,\n",
    "    adjust_for_corporate_actions,\n",
    "    handle_futures_rolls,\n",
    "    calculate_returns,\n",
    "    create_data_dictionary,\n",
    "    run_data_pipeline\n",
    ")\n",
    "\n",
    "# Import asset class mapping\n",
    "from src.asset_class_mapping import ASSET_MAPPING, create_ticker_to_asset_class_map\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "print(\"Libraries and modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f03afc",
   "metadata": {},
   "source": [
    "## 1. Find and Load Raw Data\n",
    "\n",
    "First, we need to locate our raw data file and load it using our data engineering module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec9a050a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 Excel files in ../data/raw:\n",
      "1. original_data.xlsx\n",
      "2. MSCI_Comps.xlsx\n",
      "\n",
      "Using ../data/raw/original_data.xlsx as input\n"
     ]
    }
   ],
   "source": [
    "# Look for Excel files in data/raw directory\n",
    "raw_dir = '../data/raw'\n",
    "excel_files = [f for f in os.listdir(raw_dir) if f.endswith(('.xlsx', '.xls'))]\n",
    "\n",
    "print(f\"Found {len(excel_files)} Excel files in {raw_dir}:\")\n",
    "for i, file in enumerate(excel_files):\n",
    "    print(f\"{i+1}. {file}\")\n",
    "\n",
    "# Choose the first Excel file if available\n",
    "if excel_files:\n",
    "    input_file = os.path.join(raw_dir, excel_files[0])\n",
    "    print(f\"\\nUsing {input_file} as input\")\n",
    "else:\n",
    "    input_file = input(\"Enter path to Excel file: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec936c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "raw_data = load_raw_data(input_file)\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\nRaw Data Information:\")\n",
    "print(f\"Shape: {raw_data.shape}\")\n",
    "print(f\"Date Range: {raw_data.index[0]} to {raw_data.index[-1]}\")\n",
    "print(f\"Number of Assets: {raw_data.shape[1]}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nSample Data:\")\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f1966d",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Standardization\n",
    "\n",
    "Next, we clean and standardize the data by:\n",
    "\n",
    "- Ensuring dates are in datetime format\n",
    "- Resampling to business day frequency\n",
    "- Forward-filling missing values (holidays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8b7ec3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Raw data not found. Please run the data loading cell above first.\n"
     ]
    }
   ],
   "source": [
    "# Verify raw_data is loaded\n",
    "if 'raw_data' not in locals() or raw_data is None:\n",
    "    print(\"Error: Raw data not found. Please run the data loading cell above first.\")\n",
    "else:\n",
    "    # Clean and standardize data\n",
    "    cleaned_data = clean_and_standardize(raw_data)\n",
    "    \n",
    "    if cleaned_data is not None:\n",
    "        # Display data quality statistics\n",
    "        missing_values = cleaned_data.isna().sum().sum()\n",
    "        total_values = cleaned_data.size\n",
    "        completeness = (1 - missing_values / total_values) * 100\n",
    "\n",
    "        print(\"Data Quality Statistics:\")\n",
    "        print(f\"Total Values: {total_values:,}\")\n",
    "        print(f\"Missing Values: {missing_values:,}\")\n",
    "        print(f\"Data Completeness: {completeness:.2f}%\")\n",
    "\n",
    "        # Check for any remaining NaN values\n",
    "        nan_counts = cleaned_data.isna().sum()\n",
    "        columns_with_nans = nan_counts[nan_counts > 0]\n",
    "\n",
    "        if len(columns_with_nans) > 0:\n",
    "            print(\"\\nColumns with missing values:\")\n",
    "            for col, count in columns_with_nans.items():\n",
    "                percent = (count / len(cleaned_data)) * 100\n",
    "                print(f\"- {col}: {count} missing values ({percent:.2f}%)\")\n",
    "        else:\n",
    "            print(\"\\nNo missing values found after cleaning!\")\n",
    "    else:\n",
    "        print(\"Error: Data cleaning failed. Please check the input data format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6b28dc",
   "metadata": {},
   "source": [
    "## 3. Currency Normalization\n",
    "\n",
    "Now we normalize all asset prices to USD, which is crucial for cross-asset comparison and risk budgeting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d19faeb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Normalize to USD\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m usd_data = normalize_to_usd(\u001b[43mcleaned_data\u001b[49m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Check for any significant changes after normalization\u001b[39;00m\n\u001b[32m      5\u001b[39m fx_tickers = [ticker \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m cleaned_data.columns \n\u001b[32m      6\u001b[39m               \u001b[38;5;28;01mif\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m ASSET_MAPPING.get(\u001b[33m'\u001b[39m\u001b[33mfx_crosses\u001b[39m\u001b[33m'\u001b[39m, {}).get(\u001b[33m'\u001b[39m\u001b[33mtickers\u001b[39m\u001b[33m'\u001b[39m, [])]\n",
      "\u001b[31mNameError\u001b[39m: name 'cleaned_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Normalize to USD\n",
    "usd_data = normalize_to_usd(cleaned_data)\n",
    "\n",
    "# Check for any significant changes after normalization\n",
    "fx_tickers = [ticker for ticker in cleaned_data.columns \n",
    "              if ticker in ASSET_MAPPING.get('fx_crosses', {}).get('tickers', [])]\n",
    "\n",
    "print(f\"FX tickers used for normalization: {fx_tickers}\")\n",
    "\n",
    "# For demonstration, plot one asset before and after normalization\n",
    "# Let's use a Japanese asset as an example, since it needs JPY->USD conversion\n",
    "if 'NKY Index' in cleaned_data.columns and 'USDJPY Curncy' in cleaned_data.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot original value\n",
    "    plt.subplot(1, 2, 1)\n",
    "    cleaned_data['NKY Index'].plot()\n",
    "    plt.title('Nikkei 225 (Original JPY)', fontweight='bold')\n",
    "    plt.ylabel('Price (JPY)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot USD-normalized value\n",
    "    plt.subplot(1, 2, 2)\n",
    "    usd_data['NKY Index'].plot()\n",
    "    plt.title('Nikkei 225 (USD Normalized)', fontweight='bold')\n",
    "    plt.ylabel('Price (USD Equivalent)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"USD Normalization applied successfully\")\n",
    "else:\n",
    "    print(\"Could not find suitable example for normalization visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fb26f0",
   "metadata": {},
   "source": [
    "## 4. Corporate Actions and Futures Roll Handling\n",
    "\n",
    "Now we adjust for corporate actions (e.g., dividends) and handle futures contract rolls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb2a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust for corporate actions\n",
    "adjusted_data = adjust_for_corporate_actions(usd_data)\n",
    "\n",
    "# Handle futures rolls\n",
    "processed_data = handle_futures_rolls(adjusted_data)\n",
    "\n",
    "# For demonstration, compare a futures contract before and after roll handling\n",
    "futures_contracts = ['CO1 Comdty', 'S 1 Comdty']\n",
    "for contract in futures_contracts:\n",
    "    if contract in usd_data.columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        usd_data[contract].plot()\n",
    "        plt.title(f'{contract} (Before Roll Handling)', fontweight='bold')\n",
    "        plt.ylabel('Price (USD)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        processed_data[contract].plot()\n",
    "        plt.title(f'{contract} (After Roll Handling)', fontweight='bold')\n",
    "        plt.ylabel('Price (USD)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab192de",
   "metadata": {},
   "source": [
    "## 5. Returns Calculation\n",
    "\n",
    "Calculate both daily and monthly returns from our processed price data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa81411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily returns\n",
    "daily_returns = calculate_returns(processed_data, 'D')\n",
    "\n",
    "# Calculate monthly returns\n",
    "monthly_returns = calculate_returns(processed_data, 'M')\n",
    "\n",
    "# Display statistics for daily returns\n",
    "print(\"Daily Returns Statistics:\")\n",
    "print(daily_returns.describe().T[['mean', 'std', 'min', 'max']].sort_values('std', ascending=False).head(10))\n",
    "\n",
    "# Plot distribution of daily returns for a sample asset\n",
    "if 'SPX Index' in daily_returns.columns:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    daily_returns['SPX Index'].hist(bins=50)\n",
    "    plt.title('S&P 500 Daily Returns Distribution', fontweight='bold')\n",
    "    plt.xlabel('Daily Return')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    daily_returns['SPX Index'].plot()\n",
    "    plt.title('S&P 500 Daily Returns Time Series', fontweight='bold')\n",
    "    plt.ylabel('Daily Return')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82431cc",
   "metadata": {},
   "source": [
    "## 6. Data Dictionary Creation\n",
    "\n",
    "Create a comprehensive data dictionary with metadata about each asset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf5e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data dictionary\n",
    "data_dict = create_data_dictionary(processed_data, None)  # Don't save yet, just create\n",
    "\n",
    "# Display data dictionary\n",
    "display(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74703e6",
   "metadata": {},
   "source": [
    "## 7. Run Complete Pipeline and Save Results\n",
    "\n",
    "Finally, run the complete pipeline and save all outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output directory\n",
    "output_dir = '../data/processed'\n",
    "\n",
    "# Run the complete pipeline\n",
    "results = run_data_pipeline(input_file, output_dir)\n",
    "\n",
    "# Display saved file information\n",
    "print(\"\\nFiles saved to disk:\")\n",
    "for root, dirs, files in os.walk(output_dir):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "        size_kb = os.path.getsize(full_path) / 1024\n",
    "        print(f\"  - {os.path.relpath(full_path, '..')}: {size_kb:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d31224a",
   "metadata": {},
   "source": [
    "## 8. Sample Analysis with Processed Data\n",
    "\n",
    "Let's perform a quick analysis with our processed data to verify everything is working correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ff2dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved processed data\n",
    "daily_prices = pd.read_pickle(os.path.join(output_dir, 'daily_prices.pkl'))\n",
    "daily_returns = pd.read_pickle(os.path.join(output_dir, 'daily_returns.pkl'))\n",
    "\n",
    "# Calculate correlation matrix for daily returns\n",
    "correlation = daily_returns.corr()\n",
    "\n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation, annot=False, cmap='coolwarm', center=0, linewidths=.5)\n",
    "plt.title('Correlation Matrix of Daily Returns', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze returns by asset class\n",
    "ticker_map = create_ticker_to_asset_class_map()\n",
    "asset_class_returns = {}\n",
    "\n",
    "for col in daily_returns.columns:\n",
    "    asset_info = ticker_map.get(col, {})\n",
    "    asset_class = asset_info.get('asset_class', 'unknown').replace('_', ' ').title()\n",
    "    \n",
    "    if asset_class not in asset_class_returns:\n",
    "        asset_class_returns[asset_class] = []\n",
    "    \n",
    "    asset_class_returns[asset_class].append(col)\n",
    "\n",
    "# Plot average returns by asset class\n",
    "plt.figure(figsize=(12, 6))\n",
    "asset_class_means = []\n",
    "asset_class_stds = []\n",
    "asset_class_names = []\n",
    "\n",
    "for asset_class, tickers in asset_class_returns.items():\n",
    "    if asset_class != 'Unknown' and tickers:  # Skip unknown and empty classes\n",
    "        # Calculate average return and volatility for this asset class\n",
    "        class_returns = daily_returns[tickers].mean(axis=1)\n",
    "        mean_return = class_returns.mean() * 252  # Annualize\n",
    "        std_return = class_returns.std() * np.sqrt(252)  # Annualize\n",
    "        \n",
    "        asset_class_names.append(asset_class)\n",
    "        asset_class_means.append(mean_return)\n",
    "        asset_class_stds.append(std_return)\n",
    "\n",
    "# Create a bar chart with error bars\n",
    "plt.bar(asset_class_names, asset_class_means, yerr=asset_class_stds, \n",
    "        capsize=10, color='skyblue', alpha=0.7)\n",
    "plt.title('Annualized Returns by Asset Class', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Annualized Return')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "for i, v in enumerate(asset_class_means):\n",
    "    plt.text(i, v + 0.01, f\"{v:.2%}\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8142bbff",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that our data engineering pipeline is complete, we can proceed to:\n",
    "\n",
    "1. Perform exploratory analysis to identify correlations and regime changes\n",
    "2. Design signal prototypes for each asset class\n",
    "3. Apply hierarchical risk parity within our risk budget framework\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
