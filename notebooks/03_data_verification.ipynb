{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e1c7ba",
   "metadata": {},
   "source": [
    "# Data Engineering Pipeline Verification\n",
    "\n",
    "This notebook verifies the output of the data engineering pipeline by:\n",
    "\n",
    "1. Loading and examining processed files\n",
    "2. Performing data quality checks\n",
    "3. Verifying asset class categorization\n",
    "4. Validating currency normalization\n",
    "5. Verifying return calculations\n",
    "6. Testing data consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b6d3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Ignore FutureWarnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Helper function to load pickle files\n",
    "def load_pickle(file_path):\n",
    "    try:\n",
    "        return pd.read_pickle(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Helper function to format file size\n",
    "def format_size(size_in_bytes):\n",
    "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
    "        if size_in_bytes < 1024:\n",
    "            return f\"{size_in_bytes:.2f} {unit}\"\n",
    "        size_in_bytes /= 1024\n",
    "    return f\"{size_in_bytes:.2f} GB\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b63af9",
   "metadata": {},
   "source": [
    "## 1. Load and Examine Processed Files\n",
    "\n",
    "First, we'll load all the processed files and examine their basic properties:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343fdfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all processed data files\n",
    "data_dir = Path(\"../data/processed\")\n",
    "\n",
    "# Dictionary to store our loaded data\n",
    "data = {}\n",
    "\n",
    "# Load key files\n",
    "data['daily_prices'] = load_pickle(data_dir / 'daily_prices.pkl')\n",
    "data['daily_returns'] = load_pickle(data_dir / 'daily_returns.pkl')\n",
    "data['monthly_returns'] = load_pickle(data_dir / 'monthly_returns.pkl')\n",
    "data['data_dictionary'] = pd.read_excel(data_dir / 'data_dictionary.xlsx')\n",
    "\n",
    "# Display basic information about each dataset\n",
    "for name, df in data.items():\n",
    "    if df is not None:\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "        print(f\"Number of columns: {len(df.columns)}\")\n",
    "        print(\"First few columns:\", \", \".join(df.columns[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a55129",
   "metadata": {},
   "source": [
    "## 2. Data Quality Checks\n",
    "\n",
    "Now we'll perform various data quality checks:\n",
    "\n",
    "- Missing values analysis\n",
    "- Completeness verification (should match 84.57% from pipeline logs)\n",
    "- Check for duplicates\n",
    "- Identify potential outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7653b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data completeness\n",
    "daily_prices = data['daily_prices']\n",
    "total_points = daily_prices.size\n",
    "missing_points = daily_prices.isna().sum().sum()\n",
    "completeness = (1 - missing_points / total_points) * 100\n",
    "\n",
    "print(f\"Data Completeness: {completeness:.2f}%\")\n",
    "print(f\"Total missing values: {missing_points}\")\n",
    "\n",
    "# Check for duplicated dates\n",
    "duplicates = daily_prices.index.duplicated().sum()\n",
    "print(f\"\\nDuplicated dates: {duplicates}\")\n",
    "\n",
    "# Missing values by column\n",
    "missing_by_column = daily_prices.isna().sum().sort_values(ascending=False)\n",
    "print(\"\\nTop 5 columns with most missing values:\")\n",
    "print(missing_by_column.head())\n",
    "\n",
    "# Calculate basic statistics for outlier detection\n",
    "stats = daily_prices.describe()\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85936de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create box plots to visualize outliers\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.boxplot(data=daily_prices)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Distribution of Values by Asset (with Outliers)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display extreme outliers (beyond 5 standard deviations)\n",
    "def find_extreme_outliers(df, n_std=5):\n",
    "    outliers = {}\n",
    "    for col in df.columns:\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        mask = abs(df[col] - mean) > n_std * std\n",
    "        if mask.any():\n",
    "            outliers[col] = df[mask][col]\n",
    "    return outliers\n",
    "\n",
    "extreme_outliers = find_extreme_outliers(daily_prices)\n",
    "if extreme_outliers:\n",
    "    print(\"\\nExtreme outliers (>5 std dev):\")\n",
    "    for col, values in extreme_outliers.items():\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bad876",
   "metadata": {},
   "source": [
    "## 3. Asset Class Verification\n",
    "\n",
    "Let's verify that each asset is correctly categorized according to the ASSET_MAPPING:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24c3524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import asset mapping\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.asset_class_mapping import ASSET_MAPPING, create_ticker_to_asset_class_map\n",
    "\n",
    "# Get asset mapping\n",
    "ticker_map = create_ticker_to_asset_class_map()\n",
    "\n",
    "# Compare data dictionary with asset mapping\n",
    "data_dict = data['data_dictionary']\n",
    "print(\"Asset Class Distribution:\")\n",
    "print(\"-\" * 50)\n",
    "asset_class_counts = data_dict['Asset Class'].value_counts()\n",
    "print(asset_class_counts)\n",
    "\n",
    "# Verify each asset's classification\n",
    "print(\"\\nVerifying individual asset classifications:\")\n",
    "print(\"-\" * 50)\n",
    "for _, row in data_dict.iterrows():\n",
    "    ticker = row['Ticker']\n",
    "    asset_info = ticker_map.get(ticker, {})\n",
    "    mapped_class = asset_info.get('asset_class', 'Unknown')\n",
    "    dict_class = row['Asset Class'].lower().replace(' ', '_')\n",
    "    \n",
    "    if mapped_class != dict_class:\n",
    "        print(f\"Mismatch for {ticker}:\")\n",
    "        print(f\"  Data Dictionary: {row['Asset Class']}\")\n",
    "        print(f\"  Asset Mapping: {mapped_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151d8cd6",
   "metadata": {},
   "source": [
    "## 4. Currency Normalization Validation\n",
    "\n",
    "Let's verify that the currency normalization was performed correctly by checking a few sample assets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f874522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample FX conversion check\n",
    "fx_mapping = {\n",
    "    'NKY Index': 'USDJPY Curncy',\n",
    "    'PCOMP Index': 'USDPHP Index'\n",
    "}\n",
    "\n",
    "# Load original data for comparison\n",
    "raw_data = pd.read_excel(\"../data/raw/MSCI_Comps.xlsx\", skiprows=7, index_col=0)\n",
    "\n",
    "# Check currency conversions\n",
    "for asset, fx in fx_mapping.items():\n",
    "    if asset in daily_prices.columns and asset in raw_data.columns and fx in raw_data.columns:\n",
    "        print(f\"\\nChecking currency conversion for {asset}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Get sample dates (first and last available)\n",
    "        sample_dates = [\n",
    "            max(raw_data.index[0], daily_prices.index[0]),\n",
    "            min(raw_data.index[-1], daily_prices.index[-1])\n",
    "        ]\n",
    "        \n",
    "        for date in sample_dates:\n",
    "            raw_price = raw_data.loc[date, asset]\n",
    "            converted_price = daily_prices.loc[date, asset]\n",
    "            fx_rate = raw_data.loc[date, fx]\n",
    "            \n",
    "            # Calculate expected USD price\n",
    "            if fx.startswith('USD'):\n",
    "                expected_price = raw_price / fx_rate\n",
    "            else:\n",
    "                expected_price = raw_price * fx_rate\n",
    "            \n",
    "            print(f\"\\nDate: {date}\")\n",
    "            print(f\"Original price: {raw_price:.2f}\")\n",
    "            print(f\"FX rate ({fx}): {fx_rate:.2f}\")\n",
    "            print(f\"Expected USD price: {expected_price:.2f}\")\n",
    "            print(f\"Actual USD price: {converted_price:.2f}\")\n",
    "            print(f\"Difference (%): {((converted_price - expected_price) / expected_price * 100):.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eb79fb",
   "metadata": {},
   "source": [
    "## 5. Returns Calculation Verification\n",
    "\n",
    "Let's verify the return calculations by manually computing returns for a sample of assets and comparing with the processed data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d43bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample asset for verification\n",
    "sample_asset = daily_prices.columns[0]\n",
    "\n",
    "# Calculate daily returns manually\n",
    "manual_daily_returns = daily_prices[sample_asset].pct_change()\n",
    "\n",
    "# Calculate monthly returns manually\n",
    "manual_monthly_returns = daily_prices[sample_asset].resample('ME').last().pct_change()\n",
    "\n",
    "print(f\"Returns verification for {sample_asset}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Compare daily returns\n",
    "daily_diff = manual_daily_returns - data['daily_returns'][sample_asset]\n",
    "print(\"\\nDaily Returns Verification:\")\n",
    "print(f\"Max difference: {daily_diff.abs().max():.10f}\")\n",
    "print(f\"Mean difference: {daily_diff.abs().mean():.10f}\")\n",
    "\n",
    "# Compare monthly returns\n",
    "monthly_diff = manual_monthly_returns - data['monthly_returns'][sample_asset]\n",
    "print(\"\\nMonthly Returns Verification:\")\n",
    "print(f\"Max difference: {monthly_diff.abs().max():.10f}\")\n",
    "print(f\"Mean difference: {monthly_diff.abs().mean():.10f}\")\n",
    "\n",
    "# Visualize the comparison\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(manual_daily_returns.index[-50:], manual_daily_returns[-50:], \n",
    "         label='Manual Calculation', alpha=0.7)\n",
    "plt.plot(data['daily_returns'].index[-50:], data['daily_returns'][sample_asset][-50:], \n",
    "         label='Pipeline Output', alpha=0.7)\n",
    "plt.title(f'Daily Returns Comparison - Last 50 Days\\n{sample_asset}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f78ddb6",
   "metadata": {},
   "source": [
    "## 6. Data Consistency Tests\n",
    "\n",
    "Finally, let's perform consistency checks across the processed data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8016055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check business day frequency\n",
    "print(\"Checking business day frequency...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "date_diffs = pd.Series(daily_prices.index[1:]) - pd.Series(daily_prices.index[:-1])\n",
    "irregular_intervals = date_diffs[date_diffs.dt.days > 3]\n",
    "\n",
    "if len(irregular_intervals) > 0:\n",
    "    print(\"\\nFound irregular intervals (gaps > 3 days):\")\n",
    "    for date in irregular_intervals.index:\n",
    "        print(f\"Gap between {daily_prices.index[date]} and {daily_prices.index[date + 1]}\")\n",
    "else:\n",
    "    print(\"All intervals are within expected business day frequency\")\n",
    "\n",
    "# Check for consistency between daily and monthly returns\n",
    "print(\"\\nChecking daily vs monthly returns consistency...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Calculate monthly returns from daily returns\n",
    "monthly_from_daily = (1 + data['daily_returns']).resample('ME').prod() - 1\n",
    "\n",
    "# Compare with processed monthly returns\n",
    "monthly_diff = monthly_from_daily - data['monthly_returns']\n",
    "max_diff = monthly_diff.abs().max().max()\n",
    "mean_diff = monthly_diff.abs().mean().mean()\n",
    "\n",
    "print(f\"Maximum difference: {max_diff:.10f}\")\n",
    "print(f\"Mean difference: {mean_diff:.10f}\")\n",
    "\n",
    "# Summary statistics comparison\n",
    "print(\"\\nSummary Statistics Comparison:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\\nDaily Returns Statistics:\")\n",
    "print(data['daily_returns'].describe())\n",
    "print(\"\\nMonthly Returns Statistics:\")\n",
    "print(data['monthly_returns'].describe())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
